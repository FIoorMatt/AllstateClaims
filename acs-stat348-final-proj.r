{"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"4.4.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Predictive Modeling of Automobile Insurance Claim Severity\n============================================================\nThis analysis addresses the Kaggle competition challenge to predict the financial severity of insurance claims for Allstate Insurance Company. The objective is to develop a regression model that accurately estimates the monetary loss ('loss' variable) associated with individual claims based on 130 predictive features (116 categorical, 14 continuous). Model performance is quantified using mean absolute error (MAE), measuring the average deviation between predicted and actual claim amounts. This implementation employs gradient boosting with LightGBM to optimize predictive accuracy while managing computational efficiency.","metadata":{}},{"cell_type":"code","source":"# Load libraries\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(lightgbm)\nlibrary(recipes)\nlibrary(vroom)\nlibrary(embed)\n\n\n##EDA##\ntrain_df <- vroom('train.csv')\ntest_df <- vroom('test.csv')\n\nsummary(train_df)\nstr(train_df)\n\n#Fix column error in preds fn\ncontinuous_cols <- grep(\"^cont\", names(train_df), value = TRUE)\n\nfor(col in continuous_cols) {\n  if(col %in% names(test_df)) {\n    # Ensure both are numeric\n    train_df[[col]] <- as.numeric(train_df[[col]])\n    test_df[[col]] <- as.numeric(test_df[[col]])\n  }\n}\n\ntrain_df$loss <- as.numeric(gsub(\"[^0-9.-]\", \"\", as.character(train_df$loss)))\ntrain_df$loss[is.na(train_df$loss)] <- median(train_df$loss, na.rm = TRUE)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"##Target Encoding\nmy_recipe <- recipe(loss ~ ., data = train_df) |>\n  step_rm(id) |> \n  step_other(all_nominal_predictors(), threshold = .001) |> \n  update_role(loss, new_role = \"outcome\") |>\n  step_lencode_glm(all_nominal_predictors(), outcome = vars(loss)) |> \n  step_corr(all_numeric_predictors(), threshold = 0.6) |> \n  step_normalize(all_numeric_predictors())|> \n  step_zv(all_predictors())\n\n##Test Recipe##\n#prep <- prep(my_recipe)\n#baked <- bake(prep, new_data = train_df)\n#str(baked)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"##Boosted Tree##\nboost_model <- boost_tree(tree_depth = tune(),\n                          trees = tune(),\n                          learn_rate = tune()) %>%\n  set_engine(\"lightgbm\") %>%\n  set_mode(\"regression\")\n\n\n##Workflow##\nacs_wf <- workflow() %>%\n  add_recipe(my_recipe) %>%\n  add_model(boost_model)\n\n##Tuning Grid\ngrid_of_tune <- grid_regular(tree_depth(),\n                             trees(),\n                             learn_rate(),\n                             levels = 3)\n\n##CV Split\nset.seed(294)\nfolds <- vfold_cv(train_df, v = 5)\n\n#Run CV\nCV_results <- acs_wf %>%\n  tune_grid(resamples = folds,\n            grid = grid_of_tune,\n            metrics = metric_set(mae))\n\nbestTune <- CV_results %>%\n  select_best(metric = \"mae\")\n\nfinal_wf <- acs_wf %>%\n  finalize_workflow(bestTune) %>%\n  fit(data = train_df)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"##Prediction Time##\npreds <- predict(final_wf,\n                 new_data = test_df)\n\nsubmission <- data.frame(\n  id = test_df$id,\n  loss = preds$.pred)\n\n#Remove any duplicates from submission\nsubmission <- submission[!duplicated(submission$id), ]\n\n#Write out the file to submit to Kaggle\nvroom_write(x = submission, file = \"./boostedkaggle.csv\", delim = \",\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}